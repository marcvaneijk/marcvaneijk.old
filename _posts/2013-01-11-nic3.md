---
layout: post
date: 2013-01-11
author: Marc van Eijk
title: NIC Teaming, Hyper-V switch, QoS and actual performance | part 3 – Performance
tags: Converged Fabric, Converged Network Adapter, Hyper-V QoS, IPerf, JPerf, LBFO, Marc van Eijk, Network performance, Network throughput, Networking, NIC teaming, NTttcp, Perfmon, Performance monitor, Powershell, QoS, RSS, TCP Stream, tNIC, VMQ, vNIC, Windows Server 2012
---
This blog series consists of four parts

- [NIC Teaming, Hyper-V switch, QoS and actual performance | part 1 – Theory](/2013/08/28/bmd1)
- [NIC Teaming, Hyper-V switch, QoS and actual performance | part 2 – Preparing the lab](/2013/08/30/bmd2)
- [NIC Teaming, Hyper-V switch, QoS and actual performance | part 3 – Performance](/2013/09/22/bmd3)
- [NIC Teaming, Hyper-V switch, QoS and actual performance | part 4 – Traffic classes](/2013/10/05/bmd4)

## Performance

Part 1 of this blog series explained the theory of NIC Teaming, Hyper-V switch and QoS. Theory is essential but we don’t run Hyper-V clusters in theory. We run them in production. Windows Server 2012 NIC Teaming and converged fabric allows for more bandwidth. Live migration and virtual machines are two traffic classes where more bandwidth can be useful. The following tests will look at the possible configurations to get the most bandwidth out of your Hyper-V environment on these traffic classes.

### NIC Teaming to NIC Teaming

Now that we have the tools configured we can run our first test. It is a good idea to do this one step at a time so the differences in configuration will show exactly how this influences the results.

The first step is two create a NIC team on each server and connect them directly to each other. I have used the quad port 1Gb NIC on each server to create NIC team.

<img src="/images/2013-01-11/01-nic-teaming-to-nic-teaming.png" width="720">

Each NIC team is configured in LACP / Address Hash. Running IPerf with a single stream results in a bandwidth of 113 MBytes per second.

<img src="/images/2013-01-11/02-4x1gb-stream.png" width="500">

As stated before the NIC team will force a single TCP over a single team member, so this is the expected result. Opening performance monitor during the test will verify this.

<img src="/images/2013-01-11/03-4x1gb-stream-perfmon.png" width="720">

Adding more streams will balance the sessions over the team members. After adding one TCP stream per test, all four team members were active at ten parallel TCP streams.

<img src="/images/2013-01-11/04-4x1gb-10-streams-perfmon.png" width="720">

The total bandwidth of the NIC team based on a quad port 1Gb NIC running in LACP / Address Hash is about 410 MBytes per second.

<img src="/images/2013-01-11/05-4x1gb-10-streams-jperf.png" width="720">

In the task manager we can see that RSS is working excellent. The processing of the TCP streams is correctly load balanced between the individual processors.

<img src="/images/2013-01-11/06-4x1gb-10-streams-procs.png" width="720">

### vSwitch on NIC Teaming to NIC Teaming

The previous test shows the possibilities, but running virtual machines in Hyper-V requires a Hyper-V switch. Creating a Hyper-V switch on top of the NIC team (on the server with IPerf in client mode) allows adding a vNIC on top of this Hyper-V Switch.

<img src="/images/2013-01-11/07-vswitch-on-nic-teaming-to-nic-teaming.png" width="720">

The results for this test are exactly the same as the NIC Teaming to NIC Teaming test. Initiating the TCP streams from a virtual network adapter on top of a Hyper-V switch does not influence the performance.

### vSwitch on NIC Teaming to vSwitch on NIC Teaming

In a Hyper-V cluster all cluster nodes send and receive virtual machine traffic through the Hyper-V switch. To test the performance on the receiving side a Hyper-V switch and a vNIC is also created on top of the NIC team on the server with IPerf in server mode.

<img src="/images/2013-01-11/08-vswitch-on-nic-teaming-to-vswitch-on-nic-teaming.png" width="720">

Running the same IPerf test with ten parallel TCP streams gives an interesting result. The total bandwidth is less than the previous test.

<img src="/images/2013-01-11/09-4x1gb-10-streams-vswitch-jperf.png" width="720">

On the receiving side (IPerf mode : Server) the processors are showing a different picture. Processor 0 is running at 100% and the other processors are idle.

<img src="/images/2013-01-11/10-vmq-proc0.png" width="720">

This is logical. When we created the Hyper-V switch on top on the NIC team we lost support for RSS, with VMQ automatically taking over.

You can check if VMQ is enabled with the PowerShell cmdlet `Get-NetAdapterVmq`.

<img src="/images/2013-01-11/11-get-netadaptervmq.png" width="720">

This cmdlet also shows the base VMQ processor. This is the first processor that is addressed for handling the queues. It is recommended to change this setting and allow Processor 0 for default system processing.

<img src="/images/2013-01-11/12-vmq-hardware.png" width="720">

After changing this setting in hardware settings of all the individual team members the test will show that the designated processor is now handling the traffic.

<img src="/images/2013-01-11/13-vmq-proc2.png" width="720">

There is a [good blog on VMQ](http://workinghardinit.wordpress.com/2012/02/16/dmvq-in-windows-8-hyper-v/) by Didier van Hoye ([@WorkingHardInIt](http://twitter.com/WorkingHardInIT)) that should give you some more insight. The new WS2012 feature, D-VMQ, will automatically assign the queues to the right VMs as needed based on their current activity. VMQ is enabled by default when a new virtual machine is created.

<img src="/images/2013-01-11/14-vmq-vm-settings.png" width="720">

You can find the procedure to adjust the advanced settings for VMQ in the [performance tuning guidelines for Windows Server 2012](http://msdn.microsoft.com/en-us/library/windows/hardware/jj248719.aspx).

Note In System Center Virtual Machine Manager you can use “Enable virtual network optimizations” on the Hardware Configuration tab of the virtual machine Properties to enable VMQ.
